{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74dce0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.11 environment at: /Users/sahila/Documents/Self Learning/transformers_hf/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from itertools import chain\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "!uv pip install bs4\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc26bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce15b5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "        num_rows: 810\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bff9575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': 'Ethereum Developer',\n",
       " 'prompt': 'Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.',\n",
       " 'for_devs': True,\n",
       " 'type': 'TEXT',\n",
       " 'contributor': 'ameya-2003'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c4aaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].shuffle(seed= 37).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4724ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "        num_rows: 648\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "        num_rows: 162\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## split into train, test\n",
    "dataset['train'].train_test_split(\n",
    "    train_size = 0.8, seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066fb22",
   "metadata": {},
   "source": [
    "### Creating your own dataset using a sgm file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661a0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-30 12:30:44--  https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘reuters21578.tar.gz’\n",
      "\n",
      "reuters21578.tar.gz     [  <=>               ]   7.77M  20.5MB/s    in 0.4s    \n",
      "\n",
      "2025-12-30 12:30:44 (20.5 MB/s) - ‘reuters21578.tar.gz’ saved [8150596]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c583990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/5t94392x1t1_xpdnb9_w0_0r0000gn/T/ipykernel_44885/4224340313.py:3: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=BASE_DIR/\"my_model\"/\"datasets\"/\"destination_folder\")\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "with tarfile.open(BASE_DIR/\"my_model\"/\"datasets\"/\"reuters21578.tar.gz\", 'r:gz') as tar:\n",
    "    tar.extractall(path=BASE_DIR/\"my_model\"/\"datasets\"/\"destination_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381e5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_sgm(file_path):\n",
    "    with open(file_path, 'r', encoding='latin-1') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    documents = []\n",
    "    for reuters in soup.find_all('reuters'):\n",
    "        doc = {\n",
    "            'title': reuters.title.get_text() if reuters.title else '',\n",
    "            'body': reuters.body.get_text() if reuters.body else '',\n",
    "            'topics': [topic.get_text() for topic in reuters.find_all('d')]\n",
    "        }\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "all_sgm_files = glob.glob(str(BASE_DIR/\"my_model\"/\"datasets\"/\"destination_folder\")+\"/*.sgm\")\n",
    "articles = []\n",
    "for each_file in all_sgm_files:\n",
    "    articles.append(parse_sgm(each_file))\n",
    "articles = list(chain.from_iterable(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c3aab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for article in data:\n",
    "            f.write(json.dumps(article) + \"\\n\")\n",
    "\n",
    "train_pct, val_pct = 0.8, 0.1\n",
    "# train_articles = \n",
    "# train_articles = articles\n",
    "train_articles = articles[0: int(train_pct* len(articles))]\n",
    "val_articles =  articles[int(train_pct* len(articles)): int((train_pct + val_pct)*len(articles))]\n",
    "test_articles =  articles[int((train_pct + val_pct)*len(articles)):]\n",
    "\n",
    "len(train_articles) + len(val_articles) + len(test_articles)\n",
    "\n",
    "save_to_json(train_articles, str(BASE_DIR/\"my_model\"/\"datasets\"/\"processed_output\"/\"train.json\"))\n",
    "save_to_json(test_articles, str(BASE_DIR/\"my_model\"/\"datasets\"/\"processed_output\"/\"test.json\"))\n",
    "save_to_json(val_articles, str(BASE_DIR/\"my_model\"/\"datasets\"/\"processed_output\"/\"val.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a8c31",
   "metadata": {},
   "source": [
    "## load a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idf = pl.DataFrame({'a': range(1_000_000),'b': [np.random.choice(['s','a','h','i','l']) for x in range(1_000_000)]})\n",
    "idf.write_parquet(str(BASE_DIR/\"my_model\"/\"datasets\"/\"custom_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a038cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = load_dataset(\"parquet\",data_files=str(BASE_DIR/\"my_model\"/\"datasets\"/\"custom_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The function to compute the square of a column. The output has to be formatted as a dict\n",
    "def compute_square(row)->dict:\n",
    "    return {'c': row['a']**2}\n",
    "\n",
    "my_df = my_df.map(compute_square, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset sharding for parallel processing\n",
    "print(my_df.shape)\n",
    "\n",
    "my_df['train'].shard(num_shards=6, index = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b689cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
