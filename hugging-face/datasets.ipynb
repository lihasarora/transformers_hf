{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74dce0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.11 environment at: /Users/sahila/Documents/Self Learning/transformers_hf/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from itertools import chain\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "!uv pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbc26bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce15b5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "        num_rows: 810\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bff9575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': 'Ethereum Developer',\n",
       " 'prompt': 'Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.',\n",
       " 'for_devs': True,\n",
       " 'type': 'TEXT',\n",
       " 'contributor': 'ameya-2003'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c4aaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].shuffle(seed= 37).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c4724ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "        num_rows: 648\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['act', 'prompt', 'for_devs', 'type', 'contributor'],\n",
       "        num_rows: 162\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## split into train, test\n",
    "dataset['train'].train_test_split(\n",
    "    train_size = 0.8, seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066fb22",
   "metadata": {},
   "source": [
    "### Creating your own dataset using a sgm file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661a0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-30 09:55:56--  https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘reuters21578.tar.gz’\n",
      "\n",
      "reuters21578.tar.gz     [  <=>               ]   7.77M  21.7MB/s    in 0.4s    \n",
      "\n",
      "2025-12-30 09:55:56 (21.7 MB/s) - ‘reuters21578.tar.gz’ saved [8150596]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c583990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/5t94392x1t1_xpdnb9_w0_0r0000gn/T/ipykernel_41571/4224340313.py:3: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=BASE_DIR/\"my_model\"/\"datasets\"/\"destination_folder\")\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "with tarfile.open(BASE_DIR/\"my_model\"/\"datasets\"/\"reuters21578.tar.gz\", 'r:gz') as tar:\n",
    "    tar.extractall(path=BASE_DIR/\"my_model\"/\"datasets\"/\"destination_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "381e5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_sgm(file_path):\n",
    "    with open(file_path, 'r', encoding='latin-1') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    documents = []\n",
    "    for reuters in soup.find_all('reuters'):\n",
    "        doc = {\n",
    "            'title': reuters.title.get_text() if reuters.title else '',\n",
    "            'body': reuters.body.get_text() if reuters.body else '',\n",
    "            'topics': [topic.get_text() for topic in reuters.find_all('d')]\n",
    "        }\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "all_sgm_files = glob.glob(str(BASE_DIR/\"my_model\"/\"datasets\"/\"destination_folder\")+\"/*.sgm\")\n",
    "articles = []\n",
    "for each_file in all_sgm_files:\n",
    "    articles.append(parse_sgm(each_file))\n",
    "articles = list(chain.from_iterable(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a8c31",
   "metadata": {},
   "source": [
    "## load a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9c9b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idf = pl.DataFrame({'a': range(1_000_000),'b': [np.random.choice(['s','a','h','i','l']) for x in range(1_000_000)]})\n",
    "idf.write_parquet(str(BASE_DIR/\"my_model\"/\"datasets\"/\"custom_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a038cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1000000 examples [00:00, 54054488.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "my_df = load_dataset(\"parquet\",data_files=str(BASE_DIR/\"my_model\"/\"datasets\"/\"custom_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ee9244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 1000000/1000000 [00:01<00:00, 893598.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## The function to compute the square of a column. The output has to be formatted as a dict\n",
    "def compute_square(row)->dict:\n",
    "    return {'c': row['a']**2}\n",
    "\n",
    "my_df = my_df.map(compute_square, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5cdc19eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['a', 'b', 'c'],\n",
       "    num_rows: 1000000\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae89017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (1000000, 3)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['a', 'b', 'c'],\n",
       "    num_rows: 166667\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dataset sharding for parallel processing\n",
    "print(my_df.shape)\n",
    "\n",
    "my_df['train'].shard(num_shards=6, index = 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
