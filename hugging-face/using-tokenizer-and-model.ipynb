{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d699ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80bd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Qwen/Qwen3-Reranker-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24e4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46562eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    case_1 = {'input_ids': [[40, 2948, 5538, 6832, 0], [40, 12213, 419, 773, 1753, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "    case_2 = {'input_ids': [[40, 2948, 5538, 6832, 0, 151643], [40, 12213, 419, 773, 1753, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1]]}\n",
      "    case_3 = {'input_ids': [[151643, 40, 2948, 5538, 6832, 0], [40, 12213, 419, 773, 1753, 0]], 'attention_mask': [[0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "    case_4 = {'input_ids': tensor([[151643,     40,   2948,   5538,   6832,      0],\n",
      "        [    40,  12213,    419,    773,   1753,      0]]), 'attention_mask': tensor([[0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\"I love deep learning!\",\"I hate this so much!\"]\n",
    "\n",
    "## tokenize without any additional argument\n",
    "case_1 = tokenizer(raw_inputs)\n",
    "\n",
    "## tokenizer with default padding\n",
    "case_2 = tokenizer(raw_inputs, padding = True)\n",
    "\n",
    "## tokenizer with left padding \n",
    "case_3 = tokenizer(raw_inputs, padding = True, padding_side = 'left')\n",
    "\n",
    "## returning pytorch tensor\n",
    "case_4 = tokenizer(raw_inputs, padding = True, padding_side='left', return_tensors='pt')\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    {case_1 = }\n",
    "    {case_2 = }\n",
    "    {case_3 = }\n",
    "    {case_4 = }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17019d5a",
   "metadata": {},
   "source": [
    "## What happens within the tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcdb5d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'Ġlove', 'Ġdeep', 'Ġlearning', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(raw_inputs[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d320ab84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 2948, 5538, 6832, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3f4c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [40, 2948, 5538, 6832, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prepped_ids = tokenizer.prepare_for_model(token_ids)\n",
    "model_prepped_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f08d4",
   "metadata": {},
   "source": [
    "## Tokenizer chat template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58917893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for message in messages[::-1] %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n"
     ]
    }
   ],
   "source": [
    "chat_template = tokenizer.chat_template\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2dcb9",
   "metadata": {},
   "source": [
    "### Basic conversation & System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c27f4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking what 8 plus 8 is. Let me think. Well, in math, when you add two numbers that are both 8, it's straightforward. But wait, is there any context I'm missing? Like maybe in a different context like a game or something? No, in the context of basic arithmetic, I should just give the answer directly. The result of 8 + 8 is 16. That's simple and correct. I don't have any doubts about this.\n",
      "</think>\n",
      "\n",
      "16<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = [\n",
    "    {\"role\":'system',\"content\": \"You are a chatbot expert at mathematics. You will return the final output of the question asked and no other text\"},\n",
    "    {\"role\": 'user', \"content\": \"Hello! What is 8+8?\"}\n",
    "]\n",
    "tokens_chat_template = tokenizer.apply_chat_template(sample_prompt, add_generation_prompt = True, return_tensors=\"pt\")\n",
    "\n",
    "# Define the streamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "## Generate Tokens\n",
    "model_response = model.generate(\n",
    "    tokens_chat_template,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens = 2048,\n",
    "    do_sample = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3c453",
   "metadata": {},
   "source": [
    "## Using Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50a8b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c3bc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998645782470703},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995144605636597}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I love deep learning!\",\n",
    "        \"I hate this so much\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62be5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to the store to buy some books, and I bought them all in one go. The book was a book of a very different kind than the one I had been reading for a few weeks.\n",
      "\n",
      "They were all on page four. The black book was on page two. I read it and I felt quite good about that. My wife said it was interesting, and we were going to have a conversation about it, and she said that it was a very good book.\n",
      "\n",
      "I had read this book about the book and I thought, \"Well, what do I think? What is the point of this book?\" I started doing some research, and I said, \"Well, I'm curious about it, and I can look at it and find out what's going on.\" I wanted to know what is going on with the book. I wanted to know what is going on with the whole thing.\n",
      "\n",
      "And I went to the book shop and I said, \"Well, how do I go about going through it?\" and she said, \"Well, you can go through it in two steps. First, first, you can take a look at it and see what's going on. Second, you can look at the cover. The book is one of the first things that\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\n",
    "    [\n",
    "        \"I went to the store to buy\",\n",
    "    ]\n",
    ")[0][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8b673",
   "metadata": {},
   "source": [
    "## Access the bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "259fbb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "m = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(m)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "020c6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2293, 2784, 4083,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"I love deep learning!\", padding= True, truncation = True, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c176378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.2898,  4.6176]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228996d",
   "metadata": {},
   "source": [
    "## Extract Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c26f029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(m)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(embeddings.shape)\n",
    "sentence_representation = torch.mean(embeddings, dim= 1)\n",
    "\n",
    "print(sentence_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b550c",
   "metadata": {},
   "source": [
    "## Saving a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e7ed02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
